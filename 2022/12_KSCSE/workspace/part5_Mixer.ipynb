{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b98dc65d",
   "metadata": {
    "id": "YjDiF97VlnqF"
   },
   "source": [
    " <img src=\"https://www.nvidia.com/content/dam/en-zz/Solutions/about-nvidia/logo-and-brand/01-nvidia-logo-horiz-500x200-2c50-d@2x.png\" width=300>\n",
    " \n",
    "# 계산과학공학회 인공지능 겨울학교 2022\n",
    "# [KSCSE](http://www.cse.or.kr/) 2022  GPU Tutorial  @ High1\n",
    "# Day1 - Introducion to AI \n",
    "by Hyungon Ryu | NVAITC(NVIDIA AI Tech. Center)  Korea \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97986463",
   "metadata": {
    "id": "OBf5KF3BLLfI"
   },
   "source": [
    "![](http://www.cse.or.kr/assets/img/logo_cse.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e4a87c",
   "metadata": {},
   "source": [
    "# Part5 - MLP-mixer ( revisit MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9427fd58",
   "metadata": {
    "id": "qSrvjYZqT2-R"
   },
   "source": [
    "# MLP-mixer\n",
    "\n",
    "replace MHA(multi head attention) layer and FF(Feed forward) layer with MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514394ee",
   "metadata": {
    "id": "tBzZa4SaUdlR"
   },
   "source": [
    "![](https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-20_at_12.09.16_PM_aLnxO7E.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9fb706",
   "metadata": {
    "id": "nhto88lMUz8A"
   },
   "source": [
    "see keras implementation for [MLP-mixer](https://github.com/Benjamin-Etheredge/mlp-mixer-keras/blob/main/mlp_mixer_keras/mlp_mixer.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0361e53a",
   "metadata": {
    "id": "VuZF1p7LVUKH"
   },
   "source": [
    "## MLP block\n",
    "\n",
    "it have two dense layer with bottleneck design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3302b1",
   "metadata": {
    "id": "sDhvbjbFT5I_"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import (\n",
    "    Add,\n",
    "    Dense,\n",
    "    Conv2D,\n",
    "    GlobalAveragePooling1D,\n",
    "    Layer,\n",
    "    LayerNormalization,\n",
    "    Permute,\n",
    "    Softmax,\n",
    "    Activation,\n",
    ")\n",
    "\n",
    "\n",
    "class MlpBlock(Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        hidden_dim: int,\n",
    "        activation=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(MlpBlock, self).__init__(**kwargs)\n",
    "\n",
    "        if activation is None:\n",
    "            activation = keras.activations.gelu\n",
    "\n",
    "        self.dim = dim\n",
    "        self.hidden_dim = dim\n",
    "        self.dense1 = Dense(hidden_dim)\n",
    "        self.activation = Activation(activation)\n",
    "        self.dense2 = Dense(dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        x = self.dense1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dense2(x)\n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_signature):\n",
    "        return (input_signature[0], self.dim)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(MlpBlock, self).get_config()\n",
    "        config.update({\n",
    "            'dim': self.dim,\n",
    "            'hidden_dim': self.hidden_dim\n",
    "        })\n",
    "        return config\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1caa8c",
   "metadata": {
    "id": "dCiw0PbxVrtR"
   },
   "source": [
    "### mixer block\n",
    "\n",
    "it also include transpose and skip connection ( pre/post norm option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9218d077",
   "metadata": {
    "id": "EYBpvYyWVSZk"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MixerBlock(Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_patches: int,\n",
    "        channel_dim: int,\n",
    "        token_mixer_hidden_dim: int,\n",
    "        channel_mixer_hidden_dim: int = None,\n",
    "        activation=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(MixerBlock, self).__init__(**kwargs)\n",
    "\n",
    "        self.num_patches = num_patches\n",
    "        self.channel_dim = channel_dim\n",
    "        self.token_mixer_hidden_dim = token_mixer_hidden_dim\n",
    "        self.channel_mixer_hidden_dim = channel_mixer_hidden_dim\n",
    "        self.activation = activation\n",
    "\n",
    "        if activation is None:\n",
    "            self.activation = keras.activations.gelu\n",
    "\n",
    "        if channel_mixer_hidden_dim is None:\n",
    "            channel_mixer_hidden_dim = token_mixer_hidden_dim\n",
    "\n",
    "        self.norm1 = LayerNormalization(axis=1)\n",
    "        self.permute1 = Permute((2, 1))\n",
    "        self.token_mixer = MlpBlock(num_patches, token_mixer_hidden_dim, name='token_mixer')\n",
    "\n",
    "        self.permute2 = Permute((2, 1))\n",
    "        self.norm2 = LayerNormalization(axis=1)\n",
    "        self.channel_mixer = MlpBlock(channel_dim, channel_mixer_hidden_dim, name='channel_mixer')\n",
    "\n",
    "        self.skip_connection1 = Add()\n",
    "        self.skip_connection2 = Add()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        skip_x = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.permute1(x)\n",
    "        x = self.token_mixer(x)\n",
    "\n",
    "        x = self.permute2(x)\n",
    "\n",
    "        x = self.skip_connection1([x, skip_x])\n",
    "        skip_x = x\n",
    "\n",
    "        x = self.norm2(x)\n",
    "        x = self.channel_mixer(x)\n",
    "\n",
    "        x = self.skip_connection2([x, skip_x])  # TODO need 2?\n",
    "\n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(MixerBlock, self).get_config()\n",
    "        config.update({\n",
    "            'num_patches': self.num_patches,\n",
    "            'channel_dim': self.channel_dim,\n",
    "            'token_mixer_hidden_dim': self.token_mixer_hidden_dim,\n",
    "            'channel_mixer_hidden_dim': self.channel_mixer_hidden_dim,\n",
    "            'activation': self.activation,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68b3cc2",
   "metadata": {
    "id": "5gbDQ28hV_uI"
   },
   "source": [
    "## MLP-Mixer\n",
    "original MLP-Mixer use input as patched image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e100a44",
   "metadata": {
    "id": "jGBFpX6IV_8n"
   },
   "outputs": [],
   "source": [
    "\n",
    "def MlpMixerModel(\n",
    "        input_shape: int,\n",
    "        num_classes: int,\n",
    "        num_blocks: int,\n",
    "        patch_size: int,\n",
    "        hidden_dim: int,\n",
    "        tokens_mlp_dim: int,\n",
    "        channels_mlp_dim: int = None,\n",
    "        use_softmax: bool = False,\n",
    "):\n",
    "    height, width, _ = input_shape\n",
    "\n",
    "    if channels_mlp_dim is None:\n",
    "        channels_mlp_dim = tokens_mlp_dim\n",
    "\n",
    "    num_patches = (height*width)//(patch_size**2)  # TODO verify how this behaves with same padding\n",
    "\n",
    "    inputs = keras.Input(input_shape)\n",
    "    x = inputs\n",
    "\n",
    "    x = Conv2D(hidden_dim,\n",
    "               kernel_size=patch_size,\n",
    "               strides=patch_size,\n",
    "               padding='same',\n",
    "               name='projector')(x)\n",
    "\n",
    "    x = keras.layers.Reshape([-1, hidden_dim])(x)\n",
    "\n",
    "    for _ in range(num_blocks):\n",
    "        x = MixerBlock(num_patches=num_patches,\n",
    "                       channel_dim=hidden_dim,\n",
    "                       token_mixer_hidden_dim=tokens_mlp_dim,\n",
    "                       channel_mixer_hidden_dim=channels_mlp_dim)(x)\n",
    "\n",
    "    x = GlobalAveragePooling1D()(x)  # TODO verify this global average pool is correct choice here\n",
    "\n",
    "    x = LayerNormalization(name='pre_head_layer_norm')(x)\n",
    "    x = Dense(num_classes, name='head')(x)\n",
    "\n",
    "    if use_softmax:\n",
    "        x = Softmax()(x)\n",
    "    return keras.Model(inputs, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5268b7ea",
   "metadata": {
    "id": "eiTJXnUMXAC_"
   },
   "outputs": [],
   "source": [
    "# Reshape input data from (28, 28) to (28, 28, 1)\n",
    "w, h = 28, 28\n",
    "train_images = train_images.reshape(train_images.shape[0], w, h, 1)\n",
    "test_images = test_images.reshape(test_images.shape[0], w, h, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04a68a1",
   "metadata": {
    "id": "UmGIzQrdXAHd"
   },
   "outputs": [],
   "source": [
    "model_mlp_mixer = MlpMixerModel(input_shape=train_images.shape[1:],\n",
    "                      num_classes=len(np.unique(train_labels)), \n",
    "                      num_blocks=4, \n",
    "                      patch_size=4,\n",
    "                      hidden_dim=64, \n",
    "                      tokens_mlp_dim=128,\n",
    "                      channels_mlp_dim=128,\n",
    "                      use_softmax=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd3dbbd",
   "metadata": {
    "id": "O2_34GBOZHWs"
   },
   "outputs": [],
   "source": [
    "model_mlp_mixer.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a0edb0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OZ876ZWNXAK8",
    "outputId": "95e84263-b0f6-4e69-809b-2ced09ed2155"
   },
   "outputs": [],
   "source": [
    "hist = model_mlp_mixer.fit(train_images, train_labels, validation_data=(test_images, test_labels),  epochs = 10, batch_size = 128 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1003fa5a",
   "metadata": {
    "id": "hm68SR-zXAOR"
   },
   "outputs": [],
   "source": [
    "plt.plot(hist.history['accuracy', 'val_accuracy']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5846bc",
   "metadata": {
    "id": "jxX8wJKojnr4"
   },
   "source": [
    "## additional \n",
    "\n",
    "keras also provide pre-defined and pre-trained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b03f35b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hzKS5wNcFQIQ",
    "outputId": "4e742ea4-4917-4aad-dc24-6ec8e7a41671"
   },
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = tf.pad(x_train, [[0, 0], [2,2], [2,2]])/255\n",
    "x_test = tf.pad(x_test, [[0, 0], [2,2], [2,2]])/255\n",
    "\n",
    "\n",
    "x_train = tf.expand_dims(x_train, axis=3, name=None)\n",
    "x_test = tf.expand_dims(x_test, axis=3, name=None)\n",
    "x_train = tf.repeat(x_train, 3, axis=3)\n",
    "x_test = tf.repeat(x_test, 3, axis=3)\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e18685",
   "metadata": {
    "id": "iifT2c2TFQIQ"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50V2 as resnet_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfbab5f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8hQC0s0LFQIQ",
    "outputId": "c2ae7ac9-28ed-4ddc-f7f5-8609b05c8201"
   },
   "outputs": [],
   "source": [
    "model_resnet = resnet_keras( weights = 'imagenet', include_top=False, input_shape=(32,32,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a34505",
   "metadata": {
    "id": "33ULxIYuFQIQ"
   },
   "outputs": [],
   "source": [
    "model_resnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21245e6",
   "metadata": {
    "id": "MyQ8-tLsFQIQ"
   },
   "outputs": [],
   "source": [
    "model_resnet.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87387a1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xytmz-hvFQIR",
    "outputId": "b72d3a45-8d6e-4808-ff76-2f5a57509dab"
   },
   "outputs": [],
   "source": [
    "hist = model_resnet.fit(x_train, y_train, batch_size = 64, epochs=5,verbose=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6136d8d0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fE-GgCGmFQIR",
    "outputId": "d09c8434-4b91-4018-ec81-06304172674f"
   },
   "outputs": [],
   "source": [
    "model_resnet.evaluate(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f2f25f",
   "metadata": {},
   "source": [
    "# end of jupyter part5\n",
    "### Navigation  |[Part1-reg](part1_LR.ipynb) |  [Part2-MLP](part2_MLP.ipynb) |  [Part3-CNN](part3_CNN.ipynb) |  [Part4-ResNet]( part4_resnet.ipynb) | [Part5-MLP Mixer](part5_Mixer.ipynb) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac0b87c",
   "metadata": {
    "id": "F0KVe0-hmWNj"
   },
   "source": [
    " \n",
    "\n",
    "<img src=\"https://www.nvidia.com/content/dam/en-zz/Solutions/about-nvidia/logo-and-brand/01-nvidia-logo-horiz-500x200-2c50-d@2x.png\" width=300>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
