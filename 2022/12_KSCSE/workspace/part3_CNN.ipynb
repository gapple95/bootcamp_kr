{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9258913",
   "metadata": {
    "id": "YjDiF97VlnqF"
   },
   "source": [
    " <img src=\"https://www.nvidia.com/content/dam/en-zz/Solutions/about-nvidia/logo-and-brand/01-nvidia-logo-horiz-500x200-2c50-d@2x.png\" width=300>\n",
    " \n",
    "# 계산과학공학회 인공지능 겨울학교 2022\n",
    "# [KSCSE](http://www.cse.or.kr/) 2022  GPU Tutorial  @ High1\n",
    "# Day1 - Introducion to AI \n",
    "by Hyungon Ryu | NVAITC(NVIDIA AI Tech. Center)  Korea \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10be1a6",
   "metadata": {
    "id": "OBf5KF3BLLfI"
   },
   "source": [
    "![](http://www.cse.or.kr/assets/img/logo_cse.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6267126b",
   "metadata": {},
   "source": [
    "# Part3 - CNN(Convolutional Neural Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1096e9b",
   "metadata": {
    "id": "Axoc3Vu2FQIL"
   },
   "source": [
    "## Implementing Image Classification using CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9583e430",
   "metadata": {
    "id": "RSOOrPcgFQIL"
   },
   "outputs": [],
   "source": [
    "# Import Necessary Libraries\n",
    "\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb13fec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "58cGJPV-FQIL",
    "outputId": "9fca16e8-1732-44bd-d1b2-a06ec00b060c"
   },
   "outputs": [],
   "source": [
    "# Let's Import the Dataset\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# Print array size of training dataset\n",
    "print(\"Size of Training Images: \" + str(train_images.shape))\n",
    "# Print array size of labels\n",
    "print(\"Size of Training Labels: \" + str(train_labels.shape))\n",
    "\n",
    "# Print array size of test dataset\n",
    "print(\"Size of Test Images: \" + str(test_images.shape))\n",
    "# Print array size of labels\n",
    "print(\"Size of Test Labels: \" + str(test_labels.shape))\n",
    "\n",
    "# Let's see how our outputs look\n",
    "print(\"Training Set Labels: \" + str(train_labels))\n",
    "# Data in the test dataset\n",
    "print(\"Test Set Labels: \" + str(test_labels))\n",
    "\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9885a9",
   "metadata": {
    "id": "L1i70qYUFQIM"
   },
   "source": [
    "#### preprocessnig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05bfe16",
   "metadata": {
    "id": "_RfHK8AGO6Oj"
   },
   "source": [
    "You may have noticed by now that the training set is of shape `(60000,28,28)`.\n",
    "\n",
    "In convolutional neural networks, we need to feed the data in the form of a 4D Array as follows:\n",
    "\n",
    "`(num_images, x-dims, y-dims, num_channels_per_image)`\n",
    "\n",
    "So, as our image is grayscale, we will reshape it to `(60000,28,28,1)` before passing it to our neural network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc6fa4a",
   "metadata": {
    "id": "NCxRVPEKFQIM"
   },
   "outputs": [],
   "source": [
    "# Reshape input data from (28, 28) to (28, 28, 1)\n",
    "w, h = 28, 28\n",
    "train_images = train_images.reshape(train_images.shape[0], w, h, 1)\n",
    "test_images = test_images.reshape(test_images.shape[0], w, h, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a62d9cc",
   "metadata": {
    "id": "B-IBocxEPP_j"
   },
   "source": [
    "## Defining Convolution Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915c71c1",
   "metadata": {
    "id": "o0-sQz9ZPQfX"
   },
   "source": [
    "\n",
    "Let us see how to define convolution, max pooling, and dropout layers.\n",
    "\n",
    "### Convolution Layer\n",
    "We will be using the following API to define the Convolution Layer.\n",
    "\n",
    "`tf.keras.layers.Conv2D(filters, kernel_size, padding='valid', activation=None, input_shape)`\n",
    "\n",
    "Let us briefly define the parameters:\n",
    "\n",
    " - <B>filters</B>: The dimensionality of the output space (i.e. the number of output filters in the convolution).\n",
    " - <B>kernel_size</B>: An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions.\n",
    " - <B>padding</B>: One of \"valid\" or \"same\" (case-insensitive).\n",
    " - <B>activation</B>: Activation function to use (see activations). If you don't specify anything, no activation is applied (i.e. \"linear\" activation: a(x) = x).\n",
    "\n",
    "Documentation: [Convolutional Layers](https://keras.io/layers/convolutional/)\n",
    "\n",
    "### Pooling Layer\n",
    "`tf.keras.layers.MaxPooling2D(pool_size=2)`\n",
    "\n",
    " - <B>pool_size</B>: Size of the max pooling window.\n",
    "Documentation: [Pooling Layers](https://keras.io/layers/pooling/)\n",
    "\n",
    "### Dropout\n",
    "Dropout is an approach to regularization in neural networks which helps reduce interdependent learning amongst the neurons.\n",
    "\n",
    "Simply put, dropout refers to ignoring units (i.e. neurons) during the training phase, with a certain set of neurons chosen at random. By “ignoring,\" we mean these units are not considered during a particular forward or backward pass.\n",
    "\n",
    "It is defined by the following function:\n",
    "\n",
    "`tf.keras.layers.Dropout(0.3)`\n",
    "\n",
    " - Parameter: Fraction of the input units to drop (float between 0 and 1).\n",
    "Documentation: [Dropout](https://keras.io/layers/core/#dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f18fdd4",
   "metadata": {
    "id": "XavVPcr5FQIM"
   },
   "source": [
    "## define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9411d0",
   "metadata": {
    "id": "Yq5ajF_xQTdB"
   },
   "source": [
    "Now that we are aware of the code for building a CNN, let us now build a five layer model:\n",
    "\n",
    "- Input layer: (28, 28, 1)\n",
    "  - Size of the input image\n",
    "- Convolution layers:\n",
    "  - First layer: Kernel size (3 x 3), resulting in 32 channels.\n",
    "   - Pooling of size (2 x 2) makes the layer (14 x 14 x 64)\n",
    "  - Second layer: Kernel size (3 x 3), resulting in 64 channels.\n",
    "   - Pooling of size (2 x 2) makes the layer (7 x 7 x 32)\n",
    "- Fully connected layers:\n",
    "  - Flatten the convolution layers to 1567 nodes = (7 * 7 * 32)\n",
    "  - Dense layer of size 256\n",
    "- Output layer:\n",
    "  - Dense layer with 10 classes using softmax activation\n",
    "\n",
    "  ![](https://raw.githubusercontent.com/openhackathons-org/gpubootcamp/58e1329572bebc508ba7489a9f9415d7e0592ab8/hpc_ai/ai_science_cfd/English/python/jupyter_notebook/Intro_to_DL/images/our_cnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596f9190",
   "metadata": {
    "id": "8V_453S-O3a3"
   },
   "source": [
    "Now we can define our model in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04160a97",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rLrOVl9sFQIM",
    "outputId": "8fc19f88-eb35-487e-fe2b-01d4e768ec37"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "K.clear_session()\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Must define the input shape in the first layer of the neural network\n",
    "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu', input_shape=(28,28,1))) \n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "# model.add(tf.keras.layers.Dropout(0.3))\n",
    "# Second convolution layer\n",
    "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "# Fully connected layer\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Take a look at the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1157e0",
   "metadata": {
    "id": "MRpLHQ25FQIM"
   },
   "source": [
    "#### compile the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2addf58c",
   "metadata": {
    "id": "pveuvK_ARC9-"
   },
   "source": [
    "\n",
    "Before the model is ready for training, it needs a few more settings. These are added during the model's compile step:\n",
    "\n",
    " - <B>Loss function</B> —This measures how accurate the model is during training. You want to minimize this function to \"steer\" the model in the right direction. See KERAS's [loss functions](https://keras.io/api/losses/) section\n",
    " - <B>Optimizer</B> —This is how the model is updated based on the data it sees and its loss function. See Keras [Optimizer](https://keras.io/api/optimizers/) Section\n",
    " - <B>Metrics</B> —Used to monitor the training and testing steps. The following example uses accuracy, the fraction of the images that are correctly classified. See Keras's [Metrics](https://keras.io/api/metrics/) section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79a518b",
   "metadata": {
    "id": "gPncAHOWFQIM"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bea6dde",
   "metadata": {
    "id": "Zw666qzCFQIM"
   },
   "source": [
    "#### train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dd2514",
   "metadata": {
    "id": "CM2Y8rKRRIn4"
   },
   "source": [
    "Training the neural network model requires the following steps:\n",
    "\n",
    " 1. Feed the training data to the model. In this example, the training data is in the train_images and train_labels arrays.\n",
    " 2. The model learns to associate images and labels.\n",
    " 3. You ask the model to make predictions about a test set—in this example, the  test_images array. Verify that the predictions match the labels from the test_labels array.\n",
    " \n",
    "To start training, call the model.fit method—so called because it \"fits\" the model to the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bcf872",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6j9TSW8QFQIN",
    "outputId": "349183c3-2b01-49e8-8b33-e905bb4753a8"
   },
   "outputs": [],
   "source": [
    "hist = model.fit(train_images, train_labels, validation_data=(test_images, test_labels), epochs = 10, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22757de3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "uudiR_eocTM5",
    "outputId": "5bcd63c7-b195-4ac6-d7f8-019c2058c7c1"
   },
   "outputs": [],
   "source": [
    "plt.plot(hist.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e465ae6c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OVCgBrWOFQIN",
    "outputId": "29d7eb20-7d92-46c5-9759-2c0c2cba41ea"
   },
   "outputs": [],
   "source": [
    "# Evaluating the model using the test set\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bd5cde",
   "metadata": {
    "id": "QDVAeUfWTEig"
   },
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0659eb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4djAqjZZFQIN",
    "outputId": "aaaab8f8-1859-408c-bc7e-5972d603acff"
   },
   "outputs": [],
   "source": [
    "# Making predictions from the test_images\n",
    "\n",
    "predictions = model.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef74d008",
   "metadata": {
    "id": "Y2NLsSBZFQIN"
   },
   "outputs": [],
   "source": [
    "# Reshape input data from (28, 28) to (28, 28, 1)\n",
    "w, h = 28, 28\n",
    "train_images = train_images.reshape(train_images.shape[0], w, h)\n",
    "test_images = test_images.reshape(test_images.shape[0], w, h)\n",
    "\n",
    "\n",
    "# Helper functions to plot images \n",
    "def plot_image(i, predictions_array, true_label, img):\n",
    "  predictions_array, true_label, img = predictions_array, true_label[i], img[i]\n",
    "  plt.grid(False)\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "\n",
    "  plt.imshow(img, cmap=plt.cm.binary)\n",
    "\n",
    "  predicted_label = np.argmax(predictions_array)\n",
    "  if predicted_label == true_label:\n",
    "    color = 'blue'\n",
    "  else:\n",
    "    color = 'red'\n",
    "\n",
    "  plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
    "                                100*np.max(predictions_array),\n",
    "                                class_names[true_label]),\n",
    "                                color=color)\n",
    "\n",
    "def plot_value_array(i, predictions_array, true_label):\n",
    "  predictions_array, true_label = predictions_array, true_label[i]\n",
    "  plt.grid(False)\n",
    "  plt.xticks(range(10))\n",
    "  plt.yticks([])\n",
    "  thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
    "  plt.ylim([0, 1])\n",
    "  predicted_label = np.argmax(predictions_array)\n",
    "\n",
    "  thisplot[predicted_label].set_color('red')\n",
    "  thisplot[true_label].set_color('blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1384a1bc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 729
    },
    "id": "LdTj7IQcFQIN",
    "outputId": "4a4a5464-9457-48c2-ad90-a25ea07b428c"
   },
   "outputs": [],
   "source": [
    "# Plot the first X test images, their predicted labels, and the true labels.\n",
    "# Color correct predictions in blue and incorrect predictions in red.\n",
    "num_rows = 5\n",
    "num_cols = 3\n",
    "num_images = num_rows*num_cols\n",
    "plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
    "for i in range(num_images):\n",
    "  plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
    "  plot_image(i, predictions[i], test_labels, test_images)\n",
    "  plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
    "  plot_value_array(i, predictions[i], test_labels)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff4b360",
   "metadata": {
    "id": "_U0WpOYQTT5V"
   },
   "source": [
    "### Conclusion\n",
    "Running both our models for five epochs, check the result and comparing them (your results may be slightly different):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266adf0a",
   "metadata": {
    "id": "2Ov8cRpOdcjd"
   },
   "source": [
    "# resnet\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc0dba3",
   "metadata": {
    "id": "lvPXX0bWdiOI"
   },
   "source": [
    "Residual Networks\n",
    "We discovered that learning in a convolutional neural network is hierarchical: each increase in the number of layers results in more complex features being learned by the layers. But despite this, it is shown empirically that there is a maximum threshold for depth with a traditional CNN model.\n",
    "\n",
    "In a paper titled Deep Residual Learning for Image Recognition researchers from Microsoft pointed out the following:\n",
    "![](https://raw.githubusercontent.com/openhackathons-org/gpubootcamp/58e1329572bebc508ba7489a9f9415d7e0592ab8/hpc_ai/ai_science_cfd/English/python/jupyter_notebook/Intro_to_DL/images/resnet.PNG)\n",
    "\n",
    "The failure of the 56-layer CNN could be blamed on the optimization function, initialization of the network, or the famous vanishing/exploding gradient problem. Vanishing gradients are exceptionally easy to blame for this.\n",
    "\n",
    "So what is the vanishing gradient problem? When we do backpropagation, the gradients tend to get smaller and smaller as we keep on moving backwards in the network. This means that the neurons in the earlier layers learn very slowly as compared to the neurons in the later layers in the hierarchy. The earlier layers in the network are slowest to train.\n",
    "\n",
    "Earlier layers in the network are essential because they are responsible for learning and detecting the simple patterns and are actually the building blocks of our network. If they give improper and inaccurate results, we can't expect the next layers and the complete network to perform nicely and produce accurate results.\n",
    "\n",
    "The problem of training very deep networks has been alleviated with the introduction of a new neural network layer — the residual block.\n",
    "\n",
    "Optional - The Degradation Problem\n",
    "\n",
    "The degradation problem suggests that solvers might have difficulties in approximating identity mappings by multiple nonlinear layers without the residual learning reformulation.\n",
    "\n",
    "Let us consider network A having  layers and network B having  layers. Supposing that , if network A performs poorly relative to network B, one might argue that if network A had mapped an identity function for the first  layers, then it would have performed on par with network B. But it doesn't do that due to the vanishing gradient problem, so when we use residual networks, the network gets the input along with learning on the residual, and if the input function was appropriate, it could quickly change the weights of the residual function to be zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000f57c4",
   "metadata": {
    "id": "K53YJbxEkWzc"
   },
   "source": [
    "\n",
    "## Residual Blocks\n",
    "In a residual block the activation of a layer is fast-forwarded to a deeper layer in the neural network. Residual blocks help in the flow of information from the initial layers to the final layers. This is done by the introduction of skip connections, as seen in the image below.\n",
    "\n",
    "Let us consider $H(x)$  as an underlying mapping to be fit by a few stacked layers (not necessarily the entire net), with  denoting the inputs to the first of these layers. If one hypothesizes that multiple nonlinear layers can asymptotically approximate complicated functions, then it is equivalent to hypothesize that they can asymptotically approximate the residual functions, i.e.,  (assuming that the input and output are of the same dimensions).\n",
    "\n",
    "So rather than expecting the stacked layers to approximate $H(x)$, we explicitly let these layers approximate a residual function.\n",
    "\n",
    "$F(x) = H(x) − x$.\n",
    "\n",
    "The original function thus becomes $F(x)+x$ .\n",
    "\n",
    "Although both models should be able to approximate the desired functions asymptotically, the ease of learning might be different. This reformulation is motivated by the counterintuitive phenomena about the degradation problem. As we discussed above, if the added layers can be constructed as identity mappings, a deeper model should have training error no greater than its shallow counterpart.\n",
    "![](https://raw.githubusercontent.com/openhackathons-org/gpubootcamp/58e1329572bebc508ba7489a9f9415d7e0592ab8/hpc_ai/ai_science_cfd/English/python/jupyter_notebook/Intro_to_DL/images/resblock.PNG)\n",
    "\n",
    "With this, increasing the number of layers improves accuracy. Here are some results from the paper:\n",
    "![](https://raw.githubusercontent.com/openhackathons-org/gpubootcamp/58e1329572bebc508ba7489a9f9415d7e0592ab8/hpc_ai/ai_science_cfd/English/python/jupyter_notebook/Intro_to_DL/images/stats.png)\n",
    "\n",
    "Now let us see how to write a residual block in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07983d47",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9Vg2BbbFQIN",
    "outputId": "f50f51e9-0437-4f17-d2b4-33f1298c8cbf"
   },
   "outputs": [],
   "source": [
    "# Import Necessary Libraries\n",
    "\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
    "from IPython.display import SVG\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "import scipy.misc\n",
    "from matplotlib.pyplot import imshow\n",
    "%matplotlib inline\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68851e84",
   "metadata": {
    "id": "LJdE2HatFQIO"
   },
   "outputs": [],
   "source": [
    "# Let's Import the Dataset\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dd212a",
   "metadata": {
    "id": "s9ntgFMBFQIO"
   },
   "outputs": [],
   "source": [
    "# Defining class names\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623497cd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vrBI35OWFQIO",
    "outputId": "8505dd67-745b-4ad5-e9e8-a527d5fdab69"
   },
   "outputs": [],
   "source": [
    "# Printing the Size of our Dataset\n",
    "\n",
    "# Print array size of training dataset\n",
    "print(\"Size of Training Images: \" + str(train_images.shape))\n",
    "# Print array size of labels\n",
    "print(\"Size of Training Labels: \" + str(train_labels.shape))\n",
    "\n",
    "# Print array size of test dataset\n",
    "print(\"Size of Test Images: \" + str(test_images.shape))\n",
    "# Print array size of labels\n",
    "print(\"Size of Test Labels: \" + str(test_labels.shape))\n",
    "\n",
    "# Let's see how our outputs look\n",
    "print(\"Training Set Labels: \" + str(train_labels))\n",
    "# Data in the test dataset\n",
    "print(\"Test Set Labels: \" + str(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41072094",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "P2C12HlAFQIO",
    "outputId": "242cf6d3-ae24-4e8a-c0c8-dcab25440962"
   },
   "outputs": [],
   "source": [
    "# Data Preprocessing \n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(train_images[0], cmap='gray')\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fc0499",
   "metadata": {
    "id": "DnXMBJpjFQIO"
   },
   "outputs": [],
   "source": [
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb84cfe9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 589
    },
    "id": "oVk7Ou6kFQIO",
    "outputId": "69220200-647b-46f0-bf97-cb0249c3953f"
   },
   "outputs": [],
   "source": [
    "# Let's print to verify whether the data is of the correct format.\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(class_names[train_labels[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb1056c",
   "metadata": {
    "id": "dVKyqfIgFQIO"
   },
   "outputs": [],
   "source": [
    "# Reshape input data from (28, 28) to (28, 28, 1)\n",
    "w, h = 28, 28\n",
    "train_images = train_images.reshape(train_images.shape[0], w, h, 1)\n",
    "test_images = test_images.reshape(test_images.shape[0], w, h, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124b0c71",
   "metadata": {
    "id": "8iB-LW6AFQIO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1e979c9",
   "metadata": {
    "id": "aIe_B-l-fRjL"
   },
   "source": [
    "# Model\n",
    "Let's build a resnet model with Keras and train it.\n",
    "\n",
    "We will be using two kinds of residual blocks:\n",
    "\n",
    " - Identity Block\n",
    " - Convolution Block\n",
    "\n",
    " ## Identity Block\n",
    "In the identity block we have a skip connection with no change in input, paired with a standard set of convolutional layers.\n",
    "\n",
    "![](https://raw.githubusercontent.com/openhackathons-org/gpubootcamp/58e1329572bebc508ba7489a9f9415d7e0592ab8/hpc_ai/ai_science_cfd/English/python/jupyter_notebook/Intro_to_DL/images/identity.png)\n",
    "\n",
    "## Convolution Block\n",
    "The convolution block is very similar to the identity block, but there is a convolutional layer in the skip-connection path just to change the dimension such that the dimension of the input and output matches.\n",
    "![](https://raw.githubusercontent.com/openhackathons-org/gpubootcamp/58e1329572bebc508ba7489a9f9415d7e0592ab8/hpc_ai/ai_science_cfd/English/python/jupyter_notebook/Intro_to_DL/images/conv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60506061",
   "metadata": {
    "id": "BpY6Hjo5f_yG"
   },
   "source": [
    "Let's start building the identity block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb682a81",
   "metadata": {
    "id": "28ifOtgVFQIP"
   },
   "outputs": [],
   "source": [
    "def identity_block(X, f, filters, stage, block):\n",
    "    \n",
    "    # Defining name basis\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    \n",
    "    # Retrieve filters\n",
    "    F1, F2, F3 = filters\n",
    "    \n",
    "    # A path is a block of conv followed by batch normalization and activation\n",
    "    # Save the input value\n",
    "    X_shortcut = X\n",
    "\n",
    "    # First component of main path\n",
    "    X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # Second component of main path\n",
    "    X = Conv2D(filters = F2, kernel_size = (f, f), strides = (1, 1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # Third component of main path \n",
    "    X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1, 1), padding = 'valid', name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n",
    "\n",
    "    # Final step: Add shortcut value to main path, and pass it through a RELU activation\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f276b4",
   "metadata": {
    "id": "e--4ckwPgB_t"
   },
   "source": [
    "Convolution Block\n",
    "Notice the only change we need to do is add a convolution and batch normalisation for the input data to match the output dimension.\n",
    "\n",
    "This can be done by adding the following lines :\n",
    "\n",
    "\n",
    "```\n",
    "##### SHORTCUT PATH #### \n",
    "    X_shortcut = Conv2D(filters=F3, kernel_size=(1, 1), strides=(s, s), padding='valid', name=conv_name_base + '1', kernel_initializer=glorot_uniform(seed=0))(X_shortcut)\n",
    "    X_shortcut = BatchNormalization(axis=3, name=bn_name_base + '1')(X_shortcut)\n",
    "```    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a6c3ab",
   "metadata": {
    "id": "eaFvpj1NFQIP"
   },
   "outputs": [],
   "source": [
    "def convolutional_block(X, f, filters, stage, block, s=2):\n",
    "\n",
    "    # Defining name basis\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    # Retrieve filters\n",
    "    F1, F2, F3 = filters\n",
    "\n",
    "    # Save the input value\n",
    "    X_shortcut = X\n",
    "\n",
    "    ##### MAIN PATH #####\n",
    "    # First component of main path \n",
    "    X = Conv2D(filters=F1, kernel_size=(1, 1), strides=(s, s), padding='valid', name=conv_name_base + '2a', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # Second component of main path\n",
    "    X = Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding='same', name=conv_name_base + '2b', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # Third component of main path\n",
    "    X = Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), padding='valid', name=conv_name_base + '2c', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2c')(X)\n",
    "\n",
    "    ##### SHORTCUT PATH #### \n",
    "    X_shortcut = Conv2D(filters=F3, kernel_size=(1, 1), strides=(s, s), padding='valid', name=conv_name_base + '1', kernel_initializer=glorot_uniform(seed=0))(X_shortcut)\n",
    "    X_shortcut = BatchNormalization(axis=3, name=bn_name_base + '1')(X_shortcut)\n",
    "\n",
    "    # Final step: Add shortcut value to main path, and pass it through a RELU activation\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637354f1",
   "metadata": {
    "id": "ZXxM3a-3FQIP"
   },
   "outputs": [],
   "source": [
    "def ResNet(input_shape = (28, 28, 1), classes = 10):\n",
    "\n",
    "    # Define the input as a tensor with shape input_shape\n",
    "    X_input = Input(shape=input_shape)\n",
    "\n",
    "\n",
    "    # Zero-Padding\n",
    "    X = ZeroPadding2D((3, 3))(X_input)\n",
    "\n",
    "    # Stage 1\n",
    "    X = Conv2D(64, (3, 3), strides = (2, 2), name = 'conv1', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = 'bn_conv1')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
    "\n",
    "    # Stage 2\n",
    "    X = convolutional_block(X, f = 3, filters = [64, 64, 256], stage = 2, block='a', s = 1)\n",
    "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='b')\n",
    "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='c')\n",
    "\n",
    "    # Stage 3\n",
    "    X = convolutional_block(X, f=3, filters=[128, 128, 512], stage=3, block='a', s=2)\n",
    "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='b')\n",
    "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='c')\n",
    "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='d')\n",
    "\n",
    "    # AVGPOOL\n",
    "    X = AveragePooling2D(pool_size=(2,2), padding='same')(X)\n",
    "\n",
    "    # Output layer\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(classes, activation='softmax', name='fc' + str(classes), kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "\n",
    "\n",
    "    # Create model\n",
    "    model = Model(inputs = X_input, outputs = X, name='ResNet')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f041b6d7",
   "metadata": {
    "id": "PoSGRkfcFQIP"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "K.clear_session()\n",
    "model = ResNet(input_shape = (28, 28, 1), classes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52747cd6",
   "metadata": {
    "id": "LB6AwnL_FQIP"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c142529",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qsb20pJLFQIP",
    "outputId": "a958ea6e-691d-4e3b-dca0-5faf8485e2f5"
   },
   "outputs": [],
   "source": [
    "hist = model.fit(train_images, train_labels, validation_data=(test_images, test_labels), epochs = 10, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d02676",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "jyWyhEiFdw8y",
    "outputId": "8b51674d-122a-4e4d-e885-a5e8029037ee"
   },
   "outputs": [],
   "source": [
    "plt.plot(hist.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5139c707",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "TvAvl1dxeAhv",
    "outputId": "2ba7a9e3-f463-4f7a-f64c-5168c3bcc02f"
   },
   "outputs": [],
   "source": [
    "plt.plot(hist.history['accuracy' ])\n",
    "plt.plot(hist.history[ 'val_accuracy'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25083086",
   "metadata": {
    "id": "2_LEOu4pgTCr"
   },
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c74675e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tyhXw1WUFQIP",
    "outputId": "07f0bda4-25f5-43d6-ae88-4a6d71ee98c9"
   },
   "outputs": [],
   "source": [
    "# Making predictions from the test_images\n",
    "\n",
    "predictions = model.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f7b93b",
   "metadata": {
    "id": "tq0zrdTeFQIP"
   },
   "outputs": [],
   "source": [
    "# Reshape input data from (28, 28) to (28, 28, 1)\n",
    "w, h = 28, 28\n",
    "train_images = train_images.reshape(train_images.shape[0], w, h)\n",
    "test_images = test_images.reshape(test_images.shape[0], w, h)\n",
    "\n",
    "# Helper functions to plot images \n",
    "def plot_image(i, predictions_array, true_label, img):\n",
    "  predictions_array, true_label, img = predictions_array, true_label[i], img[i]\n",
    "  plt.grid(False)\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "\n",
    "  plt.imshow(img, cmap=plt.cm.binary)\n",
    "\n",
    "  predicted_label = np.argmax(predictions_array)\n",
    "  if predicted_label == true_label:\n",
    "    color = 'blue'\n",
    "  else:\n",
    "    color = 'red'\n",
    "\n",
    "  plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
    "                                100*np.max(predictions_array),\n",
    "                                class_names[true_label]),\n",
    "                                color=color)\n",
    "\n",
    "def plot_value_array(i, predictions_array, true_label):\n",
    "  predictions_array, true_label = predictions_array, true_label[i]\n",
    "  plt.grid(False)\n",
    "  plt.xticks(range(10))\n",
    "  plt.yticks([])\n",
    "  thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
    "  plt.ylim([0, 1])\n",
    "  predicted_label = np.argmax(predictions_array)\n",
    "\n",
    "  thisplot[predicted_label].set_color('red')\n",
    "  thisplot[true_label].set_color('blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8644bd3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 729
    },
    "id": "u4k0Gf4oFQIQ",
    "outputId": "d8122a5d-4141-468f-9e65-6c4e2cff91d6"
   },
   "outputs": [],
   "source": [
    "# Plot the first X test images, their predicted labels, and the true labels.\n",
    "# Color correct predictions in blue and incorrect predictions in red.\n",
    "num_rows = 5\n",
    "num_cols = 3\n",
    "num_images = num_rows*num_cols\n",
    "plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
    "for i in range(num_images):\n",
    "  plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
    "  plot_image(i, predictions[i], test_labels, test_images)\n",
    "  plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
    "  plot_value_array(i, predictions[i], test_labels)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdbe264",
   "metadata": {
    "id": "sW2iZmwngaqO"
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95be45a",
   "metadata": {
    "id": "zDeXEFY-jepQ"
   },
   "source": [
    "Running all of our models for five epochs, and check the result and compare them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdffc42a",
   "metadata": {
    "id": "A4zjOLPSXJ5w"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d032254",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qYmvBViwXKQV",
    "outputId": "5adeeebb-ce84-425c-bb17-5a601906d610"
   },
   "outputs": [],
   "source": [
    "# Let's Import the Dataset\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e2e174",
   "metadata": {
    "id": "umOzgJSUXO-r"
   },
   "outputs": [],
   "source": [
    "# Defining class names\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188598af",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0EMJ_ByYXUsG",
    "outputId": "a55eb737-58da-403a-b186-db342bec9ba0"
   },
   "outputs": [],
   "source": [
    "# Printing the Size of our Dataset\n",
    "\n",
    "# Print array size of training dataset\n",
    "print(\"Size of Training Images: \" + str(train_images.shape))\n",
    "# Print array size of labels\n",
    "print(\"Size of Training Labels: \" + str(train_labels.shape))\n",
    "\n",
    "# Print array size of test dataset\n",
    "print(\"Size of Test Images: \" + str(test_images.shape))\n",
    "# Print array size of labels\n",
    "print(\"Size of Test Labels: \" + str(test_labels.shape))\n",
    "\n",
    "# Let's see how our outputs look\n",
    "print(\"Training Set Labels: \" + str(train_labels))\n",
    "# Data in the test dataset\n",
    "print(\"Test Set Labels: \" + str(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5fcb07",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "cqQTtIsUXYlS",
    "outputId": "2ac1b255-6a22-4b27-84a6-fbe37d69eeee"
   },
   "outputs": [],
   "source": [
    "# Data Preprocessing \n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(train_images[0], cmap='gray')\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fc091d",
   "metadata": {
    "id": "MKGrbfw-XdcG"
   },
   "outputs": [],
   "source": [
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f141d73",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 589
    },
    "id": "NGnIJhwwXik5",
    "outputId": "3656c33b-67bc-4dff-bcdd-7e97fc8441d5"
   },
   "outputs": [],
   "source": [
    "# Let's print to verify whether the data is of the correct format.\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(class_names[train_labels[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1daa55d",
   "metadata": {
    "id": "z5zxU2WlXoEn"
   },
   "outputs": [],
   "source": [
    "# Reshape input data from (28, 28) to (28, 28, 1)\n",
    "w, h = 28, 28\n",
    "train_images = train_images.reshape(train_images.shape[0], w, h, 1)\n",
    "test_images = test_images.reshape(test_images.shape[0], w, h, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6401b47a",
   "metadata": {
    "id": "qSrvjYZqT2-R"
   },
   "source": [
    "# MLP-mixer\n",
    "\n",
    "replace MHA(multi head attention) layer and FF(Feed forward) layer with MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f643073c",
   "metadata": {
    "id": "tBzZa4SaUdlR"
   },
   "source": [
    "![](https://production-media.paperswithcode.com/methods/Screen_Shot_2021-07-20_at_12.09.16_PM_aLnxO7E.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84f95c9",
   "metadata": {
    "id": "nhto88lMUz8A"
   },
   "source": [
    "see keras implementation for [MLP-mixer](https://github.com/Benjamin-Etheredge/mlp-mixer-keras/blob/main/mlp_mixer_keras/mlp_mixer.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053e9c3e",
   "metadata": {
    "id": "VuZF1p7LVUKH"
   },
   "source": [
    "## MLP block\n",
    "\n",
    "it have two dense layer with bottleneck design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea4a844",
   "metadata": {
    "id": "sDhvbjbFT5I_"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import (\n",
    "    Add,\n",
    "    Dense,\n",
    "    Conv2D,\n",
    "    GlobalAveragePooling1D,\n",
    "    Layer,\n",
    "    LayerNormalization,\n",
    "    Permute,\n",
    "    Softmax,\n",
    "    Activation,\n",
    ")\n",
    "\n",
    "\n",
    "class MlpBlock(Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        hidden_dim: int,\n",
    "        activation=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(MlpBlock, self).__init__(**kwargs)\n",
    "\n",
    "        if activation is None:\n",
    "            activation = keras.activations.gelu\n",
    "\n",
    "        self.dim = dim\n",
    "        self.hidden_dim = dim\n",
    "        self.dense1 = Dense(hidden_dim)\n",
    "        self.activation = Activation(activation)\n",
    "        self.dense2 = Dense(dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        x = self.dense1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dense2(x)\n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_signature):\n",
    "        return (input_signature[0], self.dim)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(MlpBlock, self).get_config()\n",
    "        config.update({\n",
    "            'dim': self.dim,\n",
    "            'hidden_dim': self.hidden_dim\n",
    "        })\n",
    "        return config\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90789525",
   "metadata": {
    "id": "dCiw0PbxVrtR"
   },
   "source": [
    "### mixer block\n",
    "\n",
    "it also include transpose and skip connection ( pre/post norm option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b04c00",
   "metadata": {
    "id": "EYBpvYyWVSZk"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MixerBlock(Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_patches: int,\n",
    "        channel_dim: int,\n",
    "        token_mixer_hidden_dim: int,\n",
    "        channel_mixer_hidden_dim: int = None,\n",
    "        activation=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(MixerBlock, self).__init__(**kwargs)\n",
    "\n",
    "        self.num_patches = num_patches\n",
    "        self.channel_dim = channel_dim\n",
    "        self.token_mixer_hidden_dim = token_mixer_hidden_dim\n",
    "        self.channel_mixer_hidden_dim = channel_mixer_hidden_dim\n",
    "        self.activation = activation\n",
    "\n",
    "        if activation is None:\n",
    "            self.activation = keras.activations.gelu\n",
    "\n",
    "        if channel_mixer_hidden_dim is None:\n",
    "            channel_mixer_hidden_dim = token_mixer_hidden_dim\n",
    "\n",
    "        self.norm1 = LayerNormalization(axis=1)\n",
    "        self.permute1 = Permute((2, 1))\n",
    "        self.token_mixer = MlpBlock(num_patches, token_mixer_hidden_dim, name='token_mixer')\n",
    "\n",
    "        self.permute2 = Permute((2, 1))\n",
    "        self.norm2 = LayerNormalization(axis=1)\n",
    "        self.channel_mixer = MlpBlock(channel_dim, channel_mixer_hidden_dim, name='channel_mixer')\n",
    "\n",
    "        self.skip_connection1 = Add()\n",
    "        self.skip_connection2 = Add()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        skip_x = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.permute1(x)\n",
    "        x = self.token_mixer(x)\n",
    "\n",
    "        x = self.permute2(x)\n",
    "\n",
    "        x = self.skip_connection1([x, skip_x])\n",
    "        skip_x = x\n",
    "\n",
    "        x = self.norm2(x)\n",
    "        x = self.channel_mixer(x)\n",
    "\n",
    "        x = self.skip_connection2([x, skip_x])  # TODO need 2?\n",
    "\n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(MixerBlock, self).get_config()\n",
    "        config.update({\n",
    "            'num_patches': self.num_patches,\n",
    "            'channel_dim': self.channel_dim,\n",
    "            'token_mixer_hidden_dim': self.token_mixer_hidden_dim,\n",
    "            'channel_mixer_hidden_dim': self.channel_mixer_hidden_dim,\n",
    "            'activation': self.activation,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9112ab",
   "metadata": {
    "id": "5gbDQ28hV_uI"
   },
   "source": [
    "## MLP-Mixer\n",
    "original MLP-Mixer use input as patched image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a06d5e2",
   "metadata": {
    "id": "jGBFpX6IV_8n"
   },
   "outputs": [],
   "source": [
    "\n",
    "def MlpMixerModel(\n",
    "        input_shape: int,\n",
    "        num_classes: int,\n",
    "        num_blocks: int,\n",
    "        patch_size: int,\n",
    "        hidden_dim: int,\n",
    "        tokens_mlp_dim: int,\n",
    "        channels_mlp_dim: int = None,\n",
    "        use_softmax: bool = False,\n",
    "):\n",
    "    height, width, _ = input_shape\n",
    "\n",
    "    if channels_mlp_dim is None:\n",
    "        channels_mlp_dim = tokens_mlp_dim\n",
    "\n",
    "    num_patches = (height*width)//(patch_size**2)  # TODO verify how this behaves with same padding\n",
    "\n",
    "    inputs = keras.Input(input_shape)\n",
    "    x = inputs\n",
    "\n",
    "    x = Conv2D(hidden_dim,\n",
    "               kernel_size=patch_size,\n",
    "               strides=patch_size,\n",
    "               padding='same',\n",
    "               name='projector')(x)\n",
    "\n",
    "    x = keras.layers.Reshape([-1, hidden_dim])(x)\n",
    "\n",
    "    for _ in range(num_blocks):\n",
    "        x = MixerBlock(num_patches=num_patches,\n",
    "                       channel_dim=hidden_dim,\n",
    "                       token_mixer_hidden_dim=tokens_mlp_dim,\n",
    "                       channel_mixer_hidden_dim=channels_mlp_dim)(x)\n",
    "\n",
    "    x = GlobalAveragePooling1D()(x)  # TODO verify this global average pool is correct choice here\n",
    "\n",
    "    x = LayerNormalization(name='pre_head_layer_norm')(x)\n",
    "    x = Dense(num_classes, name='head')(x)\n",
    "\n",
    "    if use_softmax:\n",
    "        x = Softmax()(x)\n",
    "    return keras.Model(inputs, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a811ee",
   "metadata": {
    "id": "eiTJXnUMXAC_"
   },
   "outputs": [],
   "source": [
    "# Reshape input data from (28, 28) to (28, 28, 1)\n",
    "w, h = 28, 28\n",
    "train_images = train_images.reshape(train_images.shape[0], w, h, 1)\n",
    "test_images = test_images.reshape(test_images.shape[0], w, h, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dbeeda",
   "metadata": {
    "id": "UmGIzQrdXAHd"
   },
   "outputs": [],
   "source": [
    "model_mlp_mixer = MlpMixerModel(input_shape=train_images.shape[1:],\n",
    "                      num_classes=len(np.unique(train_labels)), \n",
    "                      num_blocks=4, \n",
    "                      patch_size=4,\n",
    "                      hidden_dim=64, \n",
    "                      tokens_mlp_dim=128,\n",
    "                      channels_mlp_dim=128,\n",
    "                      use_softmax=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e0ba5e",
   "metadata": {
    "id": "O2_34GBOZHWs"
   },
   "outputs": [],
   "source": [
    "model_mlp_mixer.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cfe82b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OZ876ZWNXAK8",
    "outputId": "95e84263-b0f6-4e69-809b-2ced09ed2155"
   },
   "outputs": [],
   "source": [
    "hist = model_mlp_mixer.fit(train_images, train_labels, validation_data=(test_images, test_labels),  epochs = 10, batch_size = 128 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6cd75a",
   "metadata": {
    "id": "hm68SR-zXAOR"
   },
   "outputs": [],
   "source": [
    "plt.plot(hist.history['accuracy', 'val_accuracy']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3723623e",
   "metadata": {
    "id": "jxX8wJKojnr4"
   },
   "source": [
    "## additional \n",
    "\n",
    "keras also provide pre-defined and pre-trained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28ab556",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hzKS5wNcFQIQ",
    "outputId": "4e742ea4-4917-4aad-dc24-6ec8e7a41671"
   },
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = tf.pad(x_train, [[0, 0], [2,2], [2,2]])/255\n",
    "x_test = tf.pad(x_test, [[0, 0], [2,2], [2,2]])/255\n",
    "\n",
    "\n",
    "x_train = tf.expand_dims(x_train, axis=3, name=None)\n",
    "x_test = tf.expand_dims(x_test, axis=3, name=None)\n",
    "x_train = tf.repeat(x_train, 3, axis=3)\n",
    "x_test = tf.repeat(x_test, 3, axis=3)\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a009f15",
   "metadata": {
    "id": "iifT2c2TFQIQ"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50V2 as resnet_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6feb5cd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8hQC0s0LFQIQ",
    "outputId": "c2ae7ac9-28ed-4ddc-f7f5-8609b05c8201"
   },
   "outputs": [],
   "source": [
    "model_resnet = resnet_keras( weights = 'imagenet', include_top=False, input_shape=(32,32,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cb704a",
   "metadata": {
    "id": "33ULxIYuFQIQ"
   },
   "outputs": [],
   "source": [
    "model_resnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8d89a4",
   "metadata": {
    "id": "MyQ8-tLsFQIQ"
   },
   "outputs": [],
   "source": [
    "model_resnet.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d513ca6d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xytmz-hvFQIR",
    "outputId": "b72d3a45-8d6e-4808-ff76-2f5a57509dab"
   },
   "outputs": [],
   "source": [
    "hist = model_resnet.fit(x_train, y_train, batch_size = 64, epochs=5,verbose=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8a8310",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fE-GgCGmFQIR",
    "outputId": "d09c8434-4b91-4018-ec81-06304172674f"
   },
   "outputs": [],
   "source": [
    "model_resnet.evaluate(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15038b46",
   "metadata": {},
   "source": [
    "# end of jupyter part3\n",
    "### Navigation  |[Part1-reg](part1_LR.ipynb) |  [Part2-MLP](part2_MLP.ipynb) |  [Part3-CNN](part3_CNN.ipynb) |  [Part4-ResNet](part4_resnet.ipynb) | [Part5-MLP Mixer](part5_Mixer.ipynb) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd15baa",
   "metadata": {},
   "source": [
    "<img src=\"https://www.nvidia.com/content/dam/en-zz/Solutions/about-nvidia/logo-and-brand/01-nvidia-logo-horiz-500x200-2c50-d@2x.png\" width=300>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
